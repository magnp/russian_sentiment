{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Правильный файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from model import get_data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n"
     ]
    }
   ],
   "source": [
    "data = get_data('data/cleaned_data.csv')\n",
    "predictions = pd.read_csv('data/cleaned_predict.csv', header=None).values.astype('int64').astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3568,   5737,  94377],\n",
       "       [  2850,   6907,  97514],\n",
       "       [  4685,   7622, 233181]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = data[:, 1]\n",
    "y_true = np.where(y_true == '-1', '0', y_true)\n",
    "\n",
    "confusion_matrix(y_true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(y_true), y_true)\n",
    "class_weights = {0: class_weights[0],\n",
    "                 1: class_weights[1],\n",
    "                 2: class_weights[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras.layers import Bidirectional, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_util\n",
    "import io\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import math\n",
    "from functools import reduce\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "max_features = 20000\n",
    "max_len = 20\n",
    "batch_size = 32\n",
    "epochs = 6\n",
    "cur_model_name = 'multiclass_bidir_2layer_16_emb_100_drop02everywhere3_4.h5'\n",
    "\n",
    "def train_model():\n",
    "    data = get_data('data/cleaned_data.csv')\n",
    "    np.random.seed(17)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    xs, _ = get_xs(data)\n",
    "    best_words_set = data_util.construct_good_set(xs, max_features, 0)\n",
    "    data_util.sentences_to_scalars(xs, best_words_set)\n",
    "\n",
    "    train_test_split = int(0.8 * len(xs))\n",
    "\n",
    "    xs_train_scalar = keras.preprocessing.sequence.pad_sequences(xs[:train_test_split], maxlen=max_len, padding='post',\n",
    "                                                                 truncating='post')\n",
    "    xs_test_scalar = keras.preprocessing.sequence.pad_sequences(xs[train_test_split:], maxlen=max_len, padding='post',\n",
    "                                                                truncating='post')\n",
    "\n",
    "    ys_train, ys_test = get_ys(data, train_test_split)\n",
    "    ys_train = to_categorical(ys_train)\n",
    "    ys_test = to_categorical(ys_test)\n",
    "    print(ys_train[:5,:])\n",
    "\n",
    "    model = construct_model(max_features, max_len)\n",
    "\n",
    "    model.fit(\n",
    "        xs_train_scalar, ys_train,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(xs_test_scalar, ys_test),\n",
    "        epochs=epochs,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    model.save(cur_model_name)\n",
    "\n",
    "\n",
    "def get_xs(data):\n",
    "    xs_str = data[:, 0]\n",
    "    xs = np.copy(xs_str)\n",
    "    xs = [str(x).encode().decode('utf-8').split(' ') for x in xs]\n",
    "    return xs, xs_str\n",
    "\n",
    "def test_model_on_unlabeled_data():\n",
    "    data = get_data('data/cleaned_data_ok.csv')\n",
    "\n",
    "    xs, xs_str = get_xs(data)\n",
    "    best_words_set = data_util.construct_good_set(xs, max_features, 0)\n",
    "\n",
    "    data_util.sentences_to_scalars_loaded_dict(xs, best_words_set)\n",
    "\n",
    "    xs_scalar = keras.preprocessing.sequence.pad_sequences(xs, maxlen=max_len, padding='post',\n",
    "                                                                 truncating='post')\n",
    "\n",
    "    model = load_model(cur_model_name)\n",
    "    result = model.predict(xs_scalar)\n",
    "    max_indices = [np.argmax(r) for r in result]\n",
    "    # strings_and_scores = zip(xs_str, result)\n",
    "    pos = []\n",
    "    neg = []\n",
    "    neutral = []\n",
    "\n",
    "    for index, i in enumerate(max_indices):\n",
    "        if i == 0:\n",
    "            neg.append((xs_str[index], result[index]))\n",
    "            continue\n",
    "        if i == 1:\n",
    "            pos.append((xs_str[index], result[index]))\n",
    "            continue\n",
    "        if i == 2:\n",
    "            neutral.append((xs_str[index], result[index]))\n",
    "            continue\n",
    "        print(\"Something's wrong\")\n",
    "    pos = list(sorted(pos, key=lambda x: x[1][1]))\n",
    "    neg = list(sorted(neg, key=lambda x: x[1][0]))\n",
    "    neutral = list(sorted(neutral, key=lambda x: x[1][2]))\n",
    "\n",
    "    # visual_sorted = list(sorted(strings_and_scores, key=lambda x:x[1]))\n",
    "    def write_to_file(strings_and_scores, path):\n",
    "        with io.open(path, 'w', encoding='utf-8') as file_handler:\n",
    "            for item in strings_and_scores:\n",
    "                st = str(item[0]).decode('utf-8')\n",
    "                file_handler.write(u\"{}\\t{}\\n\".format(st, item[1]))\n",
    "\n",
    "    write_to_file(pos, 'data/res_pos.txt')\n",
    "    write_to_file(neg, 'data/res_neg.txt')\n",
    "    write_to_file(neutral, 'data/res_neutral.txt')\n",
    "    print(3)\n",
    "\n",
    "\n",
    "def get_ys(data, train_test_split):\n",
    "    ys = data[:, 1]\n",
    "    ys = np.where(ys == '-1', '0', ys)\n",
    "    \n",
    "    '''\n",
    "    identity = str.maketrans('', '')\n",
    "    allchars = ''.join(chr(i) for i in range(256))\n",
    "    nondigits = allchars.translate(identity, string.digits + '-')\n",
    "    # \n",
    "    #this code below is very ugly (it was build around edge cases), but it works.\n",
    "    ys = [str(y).translate(identity, nondigits) for y in ys]\n",
    "    ys = [y if len(y) > 0 else '2' for y in ys]\n",
    "    ys = [0 if y == -1 else int(\n",
    "        2 if y == '-' or (len(y) > 1 and y[1] == '-') or math.isnan(float(y))\n",
    "            else (0 if y == '-1' else y)#.translate(identity, nondigits)\n",
    "    ) for y in ys]\n",
    "    '''\n",
    "    \n",
    "    ys_train = ys[:train_test_split]\n",
    "    ys_test = ys[train_test_split:]\n",
    "    return ys_train, ys_test\n",
    "\n",
    "\n",
    "def get_data(data_path):\n",
    "    df = pd.read_csv(data_path, delimiter=\",\")\n",
    "    print(df.shape)\n",
    "    #print(df['label'].unique())\n",
    "    df = df[df['label'].isin(['-1', '1', '0', '2'])]\n",
    "    print(df.shape)\n",
    "    data = df.values[:, 1:]\n",
    "    return data\n",
    "\n",
    "\n",
    "def construct_model(max_features, max_len):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(max_features, 100, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "#train_model()\n",
    "#test_model_on_unlabeled_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91289, 3)\n",
      "(91289, 3)\n",
      "(91289, 20)\n",
      "[[ 208 5409 8757 1017   25 1358    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]] [[0.04227379 0.9492497  0.00847657]]\n"
     ]
    }
   ],
   "source": [
    "cur_model_name = 'multiclass_bidir_2layer_16_emb_100_drop02everywhere3_1.h5'\n",
    "\n",
    "def test_model_on_labeled_data(data):\n",
    "    data = get_data(data)\n",
    "\n",
    "    xs, xs_str = get_xs(data)\n",
    "    best_words_set = data_util.construct_good_set(xs, max_features, 0)\n",
    "    data_util.sentences_to_scalars_loaded_dict(xs, best_words_set)\n",
    "\n",
    "    xs_scalar = keras.preprocessing.sequence.pad_sequences(xs, maxlen=max_len, padding='post',\n",
    "                                                                 truncating='post')\n",
    "\n",
    "    ys = data[:, 1]\n",
    "    #print(np.unique(ys.astype('str')))\n",
    "    ys = [0 if y == '-1' else int(0 if math.isnan(float(y)) else y) for y in ys]\n",
    "    model = load_model(cur_model_name)\n",
    "    print(xs_scalar.shape)\n",
    "    print(xs_scalar[:1,:], model.predict(xs_scalar[:1,:]))\n",
    "    result = np.argmax(model.predict(xs_scalar), axis=1)\n",
    "    np.savetxt('data/cleaned_predict_test_1.csv', result, delimiter=',')\n",
    "    #print(result)\n",
    "    '''\n",
    "    visual = zip(xs_str, result)\n",
    "    predicted_and_true = zip(result, ys)\n",
    "    true_pos = reduce(lambda a, b: a + (1 if b[0] == b[1] and b[0] == 1 else 0), predicted_and_true, 0)\n",
    "    false_pos = reduce(lambda a, b: a + (1 if b[0] != b[1] and b[1] == 0 else 0), predicted_and_true, 0)\n",
    "    false_neg = reduce(lambda a, b: a + (1 if b[0] != b[1] and b[1] == 1 else 0), predicted_and_true, 0)\n",
    "    \n",
    "    precision = true_pos / np.float(true_pos + false_neg)\n",
    "    recall = true_pos / np.float(true_pos + false_pos)\n",
    "    #precision = precision_score(ys, result)\n",
    "    #recall = recall_score(ys, result)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    '''\n",
    "    \n",
    "test_model_on_labeled_data('data/cleaned_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n"
     ]
    }
   ],
   "source": [
    "data = get_data('data/cleaned_data.csv')\n",
    "np.random.seed(17)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "#best_words_set = data_util.construct_good_set(xs, max_features, 0)\n",
    "#data_util.sentences_to_scalars(xs, best_words_set)\n",
    "\n",
    "train_test_split = int(0.8 * data.shape[0])\n",
    "\n",
    "train_data = data[:train_test_split,:]\n",
    "test_data = data[train_test_split:,:]\n",
    "\n",
    "train_data = pd.DataFrame(train_data, columns=['text', 'label'])\n",
    "test_data = pd.DataFrame(test_data, columns=['text', 'label'])\n",
    "\n",
    "train_data.to_csv('data/cleaned_data_train.csv')\n",
    "test_data.to_csv('data/cleaned_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ самой первой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  3568,   5737,  94377],\n",
       "       [  2850,   6907,  97514],\n",
       "       [  4685,   7622, 233181]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data('data/cleaned_data.csv')\n",
    "predictions = pd.read_csv('data/cleaned_predict.csv', header=None).values.astype('int64').astype('str')\n",
    "\n",
    "y_true = data[:, 1]\n",
    "y_true = np.where(y_true == '-1', '0', y_true)\n",
    "\n",
    "confusion_matrix(y_true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.sum(np.diag(confusion_matrix(y_true, predictions))) / y_true.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5338170760295416"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ модели номер 1 - тренируем с поправкой на классовые веса в течение 10 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 83081,  12259,   8342],\n",
       "       [ 11373,  85006,  10892],\n",
       "       [ 27400,  34636, 183452]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data('data/cleaned_data.csv')\n",
    "predictions = pd.read_csv('data/cleaned_predict_1.csv', header=None).values.astype('int64').astype('str')\n",
    "\n",
    "y_true = data[:, 1]\n",
    "y_true = np.where(y_true == '-1', '0', y_true)\n",
    "\n",
    "confusion_matrix(y_true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7701740203005427"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(np.diag(confusion_matrix(y_true, predictions))) / y_true.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ модели номер 2 - немного апгрейдим batch_size, lr и количество эпох=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 73940,  19573,  10169],\n",
       "       [ 21329,  71334,  14608],\n",
       "       [ 38983,  50061, 156444]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data('data/cleaned_data.csv')\n",
    "predictions = pd.read_csv('data/cleaned_predict_2.csv', header=None).values.astype('int64').astype('str')\n",
    "\n",
    "y_true = data[:, 1]\n",
    "y_true = np.where(y_true == '-1', '0', y_true)\n",
    "\n",
    "confusion_matrix(y_true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6610230018775701"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(np.diag(confusion_matrix(y_true, predictions))) / y_true.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ модели номер 3 - как модель 3, но количество эпох=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 71056,  21488,  11138],\n",
       "       [ 21003,  70140,  16128],\n",
       "       [ 36869,  51033, 157586]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data('data/cleaned_data.csv')\n",
    "predictions = pd.read_csv('data/cleaned_predict_3.csv', header=None).values.astype('int64').astype('str')\n",
    "\n",
    "y_true = data[:, 1]\n",
    "y_true = np.where(y_true == '-1', '0', y_true)\n",
    "\n",
    "confusion_matrix(y_true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6545906261707428"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(np.diag(confusion_matrix(y_true, predictions))) / y_true.shape[0]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Новые эмбеддинги\n",
    "\n",
    "https://rusvectores.org/ru/about/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('data/news_upos_cbow_300_2_2017.bin', binary=True)\n",
    "model.save_word2vec_format('data/news_upos_cbow_300_2_2017.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1a2fdd3d30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
