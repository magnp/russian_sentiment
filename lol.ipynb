{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from model import get_data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n"
     ]
    }
   ],
   "source": [
    "data = get_data('data/cleaned_data.csv')\n",
    "predictions = pd.read_csv('data/cleaned_predict.csv', header=None).values.astype('int64').astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3568,   5737,  94377],\n",
       "       [  2850,   6907,  97514],\n",
       "       [  4685,   7622, 233181]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = data[:, 1]\n",
    "y_true = np.where(y_true == '-1', '0', y_true)\n",
    "\n",
    "confusion_matrix(y_true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(y_true), y_true)\n",
    "class_weights = {0: class_weights[0],\n",
    "                 1: class_weights[1],\n",
    "                 2: class_weights[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456770, 3)\n",
      "(456441, 3)\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 20, 32)            14976     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,021,347\n",
      "Trainable params: 2,021,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 365152 samples, validate on 91289 samples\n",
      "Epoch 1/10\n",
      "  5856/365152 [..............................] - ETA: 25:36 - loss: 1.0279 - acc: 0.5174"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fb5d1155db43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;31m# test_model_on_labeled_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;31m# test_model_on_unlabeled_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fb5d1155db43>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs_test_scalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras.layers import Bidirectional, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_util\n",
    "import io\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import math\n",
    "from functools import reduce\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "max_features = 20000\n",
    "max_len = 20\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "cur_model_name = 'multiclass_bidir_2layer_16_emb_100_drop02everywhere3_1.h5'\n",
    "\n",
    "def train_model():\n",
    "    data = get_data('data/cleaned_data.csv')\n",
    "    np.random.seed(17)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    xs, _ = get_xs(data)\n",
    "    best_words_set = data_util.construct_good_set(xs, max_features, 0)\n",
    "    data_util.sentences_to_scalars(xs, best_words_set)\n",
    "\n",
    "    train_test_split = int(0.8 * len(xs))\n",
    "\n",
    "    xs_train_scalar = keras.preprocessing.sequence.pad_sequences(xs[:train_test_split], maxlen=max_len, padding='post',\n",
    "                                                                 truncating='post')\n",
    "    xs_test_scalar = keras.preprocessing.sequence.pad_sequences(xs[train_test_split:], maxlen=max_len, padding='post',\n",
    "                                                                truncating='post')\n",
    "\n",
    "    ys_train, ys_test = get_ys(data, train_test_split)\n",
    "    ys_train = to_categorical(ys_train)\n",
    "    ys_test = to_categorical(ys_test)\n",
    "    print(ys_train[:5,:])\n",
    "\n",
    "    model = construct_model(max_features, max_len)\n",
    "\n",
    "    model.fit(\n",
    "        xs_train_scalar, ys_train,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(xs_test_scalar, ys_test),\n",
    "        epochs=epochs,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    model.save(cur_model_name)\n",
    "\n",
    "\n",
    "def get_xs(data):\n",
    "    xs_str = data[:, 0]\n",
    "    xs = np.copy(xs_str)\n",
    "    xs = [str(x).encode().decode('utf-8').split(' ') for x in xs]\n",
    "    return xs, xs_str\n",
    "\n",
    "\n",
    "def test_model_on_labeled_data():\n",
    "    data = get_data('data/cleaned_data.csv')\n",
    "\n",
    "    xs, xs_str = get_xs(data)\n",
    "    best_words_set = data_util.construct_good_set(xs, max_features, 0)\n",
    "    data_util.sentences_to_scalars_loaded_dict(xs, best_words_set)\n",
    "\n",
    "    xs_scalar = keras.preprocessing.sequence.pad_sequences(xs, maxlen=max_len, padding='post',\n",
    "                                                                 truncating='post')\n",
    "\n",
    "    ys = data[:, 1]\n",
    "    #print(np.unique(ys.astype('str')))\n",
    "    ys = [0 if y == '-1' else int(0 if math.isnan(float(y)) else y) for y in ys]\n",
    "    model = load_model(cur_model_name)\n",
    "    print(xs_scalar.shape)\n",
    "    print(xs_scalar[:1,:], model.predict(xs_scalar[:1,:]))\n",
    "    result = np.argmax(model.predict(xs_scalar), axis=1)\n",
    "    np.savetxt('data/cleaned_predict.csv', result, delimiter=',')\n",
    "    print(result)\n",
    "    visual = zip(xs_str, result)\n",
    "    predicted_and_true = zip(result, ys)\n",
    "    true_pos = reduce(lambda a, b: a + (1 if b[0] == b[1] and b[0] == 1 else 0), predicted_and_true, 0)\n",
    "    false_pos = reduce(lambda a, b: a + (1 if b[0] != b[1] and b[1] == 0 else 0), predicted_and_true, 0)\n",
    "    false_neg = reduce(lambda a, b: a + (1 if b[0] != b[1] and b[1] == 1 else 0), predicted_and_true, 0)\n",
    "    \n",
    "    precision = true_pos / np.float(true_pos + false_neg)\n",
    "    recall = true_pos / np.float(true_pos + false_pos)\n",
    "    #precision = precision_score(ys, result)\n",
    "    #recall = recall_score(ys, result)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "\n",
    "def test_model_on_unlabeled_data():\n",
    "    data = get_data('data/cleaned_data_ok.csv')\n",
    "\n",
    "    xs, xs_str = get_xs(data)\n",
    "    best_words_set = data_util.construct_good_set(xs, max_features, 0)\n",
    "\n",
    "    data_util.sentences_to_scalars_loaded_dict(xs, best_words_set)\n",
    "\n",
    "    xs_scalar = keras.preprocessing.sequence.pad_sequences(xs, maxlen=max_len, padding='post',\n",
    "                                                                 truncating='post')\n",
    "\n",
    "    model = load_model(cur_model_name)\n",
    "    result = model.predict(xs_scalar)\n",
    "    max_indices = [np.argmax(r) for r in result]\n",
    "    # strings_and_scores = zip(xs_str, result)\n",
    "    pos = []\n",
    "    neg = []\n",
    "    neutral = []\n",
    "\n",
    "    for index, i in enumerate(max_indices):\n",
    "        if i == 0:\n",
    "            neg.append((xs_str[index], result[index]))\n",
    "            continue\n",
    "        if i == 1:\n",
    "            pos.append((xs_str[index], result[index]))\n",
    "            continue\n",
    "        if i == 2:\n",
    "            neutral.append((xs_str[index], result[index]))\n",
    "            continue\n",
    "        print(\"Something's wrong\")\n",
    "    pos = list(sorted(pos, key=lambda x: x[1][1]))\n",
    "    neg = list(sorted(neg, key=lambda x: x[1][0]))\n",
    "    neutral = list(sorted(neutral, key=lambda x: x[1][2]))\n",
    "\n",
    "    # visual_sorted = list(sorted(strings_and_scores, key=lambda x:x[1]))\n",
    "    def write_to_file(strings_and_scores, path):\n",
    "        with io.open(path, 'w', encoding='utf-8') as file_handler:\n",
    "            for item in strings_and_scores:\n",
    "                st = str(item[0]).decode('utf-8')\n",
    "                file_handler.write(u\"{}\\t{}\\n\".format(st, item[1]))\n",
    "\n",
    "    write_to_file(pos, 'data/res_pos.txt')\n",
    "    write_to_file(neg, 'data/res_neg.txt')\n",
    "    write_to_file(neutral, 'data/res_neutral.txt')\n",
    "    print(3)\n",
    "\n",
    "\n",
    "def get_ys(data, train_test_split):\n",
    "    ys = data[:, 1]\n",
    "    ys = np.where(ys == '-1', '0', ys)\n",
    "    \n",
    "    '''\n",
    "    identity = str.maketrans('', '')\n",
    "    allchars = ''.join(chr(i) for i in range(256))\n",
    "    nondigits = allchars.translate(identity, string.digits + '-')\n",
    "    # \n",
    "    #this code below is very ugly (it was build around edge cases), but it works.\n",
    "    ys = [str(y).translate(identity, nondigits) for y in ys]\n",
    "    ys = [y if len(y) > 0 else '2' for y in ys]\n",
    "    ys = [0 if y == -1 else int(\n",
    "        2 if y == '-' or (len(y) > 1 and y[1] == '-') or math.isnan(float(y))\n",
    "            else (0 if y == '-1' else y)#.translate(identity, nondigits)\n",
    "    ) for y in ys]\n",
    "    '''\n",
    "    \n",
    "    ys_train = ys[:train_test_split]\n",
    "    ys_test = ys[train_test_split:]\n",
    "    return ys_train, ys_test\n",
    "\n",
    "\n",
    "def get_data(data_path):\n",
    "    df = pd.read_csv(data_path, delimiter=\",\")\n",
    "    print(df.shape)\n",
    "    #print(df['label'].unique())\n",
    "    df = df[df['label'].isin(['-1', '1', '0', '2'])]\n",
    "    print(df.shape)\n",
    "    data = df.values[:, 1:]\n",
    "    return data\n",
    "\n",
    "\n",
    "def construct_model(max_features, max_len):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(max_features, 100, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "train_model()\n",
    "# test_model_on_labeled_data()\n",
    "# test_model_on_unlabeled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
